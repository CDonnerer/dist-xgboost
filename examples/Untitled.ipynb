{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee4852ed-39d2-48b6-9c5d-6b762d01a515",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from xgboost_distribution import XGBDistribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a144c3f-7bb7-409b-9caf-d90cbbf7d789",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_doc = XGBDistribution.__doc__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c7f3848-8a69-407b-ac69-0142d867946d",
   "metadata": {},
   "outputs": [],
   "source": [
    "KV_REGEX = re.compile(r\"^[^\\s].*$\", flags=re.M)\n",
    "PARAM_KEY_REGEX = re.compile(r\"^(?P<name>.*?)(?:\\s*:\\s*(?P<type>.*?))?$\")\n",
    "PARAM_OPTIONAL_REGEX = re.compile(r\"(?P<type>.*?)(?:, optional|\\(optional\\))$\")\n",
    "\n",
    "# numpydoc format has no formal grammar for this,\n",
    "# but we can make some educated guesses...\n",
    "PARAM_DEFAULT_REGEX = re.compile(\n",
    "    r\"[Dd]efault(?: is | = |: |s to |)\\s*(?P<value>[\\w\\-\\.]+)\"\n",
    ")\n",
    "\n",
    "RETURN_KEY_REGEX = re.compile(r\"^(?:(?P<name>.*?)\\s*:\\s*)?(?P<type>.*?)$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d470722b-d4b1-41a2-9afa-f934f9778749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MY_REGEX = re.compile(r\"(?:, \\(objective\\))\", flags=re.M)\n",
    "MY_REGEX.findall(class_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "5eef5546-bca3-4e2d-932e-444630931420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    n_jobs : int\n",
      "        Number of parallel threads used to run xgboost.  When used with other Scikit-Learn\n",
      "        algorithms like grid search, you may choose which algorithm to parallelize and\n",
      "        balance the threads.  Creating thread contention will significantly slow down both\n",
      "        algorithms.\n"
     ]
    }
   ],
   "source": [
    "s = (\n",
    "    \"(debug).\\n\"\n",
    "    \"    objective : string or callable\\n\"\n",
    "    \"        Specify the learning task and the corresponding learning objective or\\n\"\n",
    "    \"        a custom objective function to be used (see note below).\\n\"\n",
    "    \"    booster: string\\n\"\n",
    "    \"        Specify which booster to use: gbtree, gblinear or dart.\\n\"\n",
    "    \"    tree_method: string\\n\"\n",
    "    \"        Specify which tree method to use.  Default to auto.\"\n",
    ")\n",
    "\n",
    "param = \"n_jobs\"\n",
    "\n",
    "match = re.findall(fr\"^\\s+{param}.*?[\\.]$\", class_doc, re.M | re.DOTALL) #, )\n",
    "\n",
    "#match = re.findall(fr\"^\\s+{param}.*(?:(?!\\n    [a-zA-Z]))\", s, re.M | re.DOTALL) #, )\n",
    "\n",
    "\n",
    "print(match[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "12e2eeb6-27b1-4f6e-bcf4-15ea698a76c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implementation of XGBoost to estimate distributions (in scikit-learn API).\n",
      "\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "\n",
      "    distribution : {'negative-binomial', 'log-normal', 'exponential', 'poisson', 'normal', 'laplace'}, default='normal'\n",
      "        Which distribution to estimate. Available choices:\n",
      "    \n",
      "                - \"exponential\" - parameters: ('scale',)\n",
      "\n",
      "                - \"laplace\" - parameters: ('loc', 'scale')\n",
      "\n",
      "                - \"log-normal\" - parameters: ('scale', 's')\n",
      "\n",
      "                - \"negative-binomial\" - parameters: ('n', 'p')\n",
      "\n",
      "                - \"normal\" - parameters: ('loc', 'scale')\n",
      "\n",
      "                - \"poisson\" - parameters: ('mu',)\n",
      "\n",
      "        Please see `scipy.stats` for a full description of the parameters of\n",
      "        each distribution: https://docs.scipy.org/doc/scipy/reference/stats.html\n",
      "    \n",
      "    natural_gradient : bool, default=True\n",
      "        Whether or not natural gradients should be used.\n",
      "    n_estimators : int\n",
      "        Number of gradient boosted trees.  Equivalent to number of boosting\n",
      "        rounds.\n",
      "\n",
      "    max_depth : int\n",
      "        Maximum tree depth for base learners.\n",
      "    learning_rate : float\n",
      "        Boosting learning rate (xgb's \"eta\")\n",
      "    verbosity : int\n",
      "        The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n",
      "    objective : string or callable\n",
      "        Specify the learning task and the corresponding learning objective or\n",
      "        a custom objective function to be used (see note below).\n",
      "    booster: string\n",
      "        Specify which booster to use: gbtree, gblinear or dart.\n",
      "    tree_method: string\n",
      "        Specify which tree method to use.  Default to auto.  If this parameter\n",
      "        is set to default, XGBoost will choose the most conservative option\n",
      "        available.  It's recommended to study this option from parameters\n",
      "        document.\n",
      "    n_jobs : int\n",
      "        Number of parallel threads used to run xgboost.  When used with other Scikit-Learn\n",
      "        algorithms like grid search, you may choose which algorithm to parallelize and\n",
      "        balance the threads.  Creating thread contention will significantly slow down both\n",
      "        algorithms.\n",
      "    gamma : float\n",
      "        Minimum loss reduction required to make a further partition on a leaf\n",
      "        node of the tree.\n",
      "    min_child_weight : float\n",
      "        Minimum sum of instance weight(hessian) needed in a child.\n",
      "    max_delta_step : float\n",
      "        Maximum delta step we allow each tree's weight estimation to be.\n",
      "    subsample : float\n",
      "        Subsample ratio of the training instance.\n",
      "    colsample_bytree : float\n",
      "        Subsample ratio of columns when constructing each tree.\n",
      "    colsample_bylevel : float\n",
      "        Subsample ratio of columns for each level.\n",
      "    colsample_bynode : float\n",
      "        Subsample ratio of columns for each split.\n",
      "    reg_alpha : float (xgb's alpha)\n",
      "        L1 regularization term on weights\n",
      "    reg_lambda : float (xgb's lambda)\n",
      "        L2 regularization term on weights\n",
      "    scale_pos_weight : float\n",
      "        Balancing of positive and negative weights.\n",
      "    base_score:\n",
      "        The initial prediction score of all instances, global bias.\n",
      "    random_state : int\n",
      "        Random number seed.\n",
      "\n",
      "        .. note::\n",
      "\n",
      "           Using gblinear booster with shotgun updater is nondeterministic as\n",
      "           it uses Hogwild algorithm.\n",
      "\n",
      "    missing : float, default np.nan\n",
      "        Value in the data which needs to be present as a missing value.\n",
      "    num_parallel_tree: int\n",
      "        Used for boosting random forest.\n",
      "    monotone_constraints : str\n",
      "        Constraint of variable monotonicity.  See tutorial for more\n",
      "        information.\n",
      "    interaction_constraints : str\n",
      "        Constraints for interaction representing permitted interactions.  The\n",
      "        constraints must be specified in the form of a nest list, e.g. [[0, 1],\n",
      "        [2, 3, 4]], where each inner list is a group of indices of features\n",
      "        that are allowed to interact with each other.  See tutorial for more\n",
      "        information\n",
      "    importance_type: string, default \"gain\"\n",
      "        The feature importance type for the feature_importances\\_ property:\n",
      "        either \"gain\", \"weight\", \"cover\", \"total_gain\" or \"total_cover\".\n",
      "    gpu_id :\n",
      "        Device ordinal.\n",
      "    validate_parameters :\n",
      "        Give warnings for unknown parameter.\n",
      "\n",
      "    \\*\\*kwargs : dict, optional\n",
      "        Keyword arguments for XGBoost Booster object.  Full documentation of\n",
      "        parameters can be found here:\n",
      "        https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst.\n",
      "        Attempting to set a parameter via the constructor args and \\*\\*kwargs\n",
      "        dict simultaneously will result in a TypeError.\n",
      "\n",
      "        .. note:: \\*\\*kwargs unsupported by scikit-learn\n",
      "\n",
      "            \\*\\*kwargs is unsupported by scikit-learn.  We do not guarantee\n",
      "            that parameters passed via this argument will interact properly\n",
      "            with scikit-learn.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(class_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "bee85261-60fb-4589-b4bc-22a15a5fc835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implementation of XGBoost to estimate distributions (in scikit-learn API).\n",
      "\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "\n",
      "    distribution : {'negative-binomial', 'log-normal', 'exponential', 'poisson', 'normal', 'laplace'}, default='normal'\n",
      "        Which distribution to estimate. Available choices:\n",
      "    \n",
      "                - \"exponential\" - parameters: ('scale',)\n",
      "\n",
      "                - \"laplace\" - parameters: ('loc', 'scale')\n",
      "\n",
      "                - \"log-normal\" - parameters: ('scale', 's')\n",
      "\n",
      "                - \"negative-binomial\" - parameters: ('n', 'p')\n",
      "\n",
      "                - \"normal\" - parameters: ('loc', 'scale')\n",
      "\n",
      "                - \"poisson\" - parameters: ('mu',)\n",
      "\n",
      "        Please see `scipy.stats` for a full description of the parameters of\n",
      "        each distribution: https://docs.scipy.org/doc/scipy/reference/stats.html\n",
      "    \n",
      "    natural_gradient : bool, default=True\n",
      "        Whether or not natural gradients should be used.\n",
      "    n_estimators : int\n",
      "        Number of gradient boosted trees.  Equivalent to number of boosting\n",
      "        rounds.\n",
      "\n",
      "    max_depth : int\n",
      "        Maximum tree depth for base learners.\n",
      "    learning_rate : float\n",
      "        Boosting learning rate (xgb's \"eta\")\n",
      "    verbosity : int\n",
      "        The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n",
      "    objective : string or callable\n",
      "        Specify the learning task and the corresponding learning objective or\n",
      "        a custom objective function to be used (see note below).\n",
      "    booster: string\n",
      "        Specify which booster to use: gbtree, gblinear or dart.\n",
      "    tree_method: string\n",
      "        Specify which tree method to use.  Default to auto.  If this parameter\n",
      "        is set to default, XGBoost will choose the most conservative option\n",
      "        available.  It's recommended to study this option from parameters\n",
      "        document.\n",
      "    n_jobs : int\n",
      "        Number of parallel threads used to run xgboost.  When used with other Scikit-Learn\n",
      "        algorithms like grid search, you may choose which algorithm to parallelize and\n",
      "        balance the threads.  Creating thread contention will significantly slow down both\n",
      "        algorithms.\n",
      "    gamma : float\n",
      "        Minimum loss reduction required to make a further partition on a leaf\n",
      "        node of the tree.\n",
      "    min_child_weight : float\n",
      "        Minimum sum of instance weight(hessian) needed in a child.\n",
      "    max_delta_step : float\n",
      "        Maximum delta step we allow each tree's weight estimation to be.\n",
      "    subsample : float\n",
      "        Subsample ratio of the training instance.\n",
      "    colsample_bytree : float\n",
      "        Subsample ratio of columns when constructing each tree.\n",
      "    colsample_bylevel : float\n",
      "        Subsample ratio of columns for each level.\n",
      "    colsample_bynode : float\n",
      "        Subsample ratio of columns for each split.\n",
      "    reg_alpha : float (xgb's alpha)\n",
      "        L1 regularization term on weights\n",
      "    reg_lambda : float (xgb's lambda)\n",
      "        L2 regularization term on weights\n",
      "    scale_pos_weight : float\n",
      "        Balancing of positive and negative weights.\n",
      "    base_score:\n",
      "        The initial prediction score of all instances, global bias.\n",
      "    random_state : int\n",
      "        Random number seed.\n",
      "\n",
      "        .. note::\n",
      "\n",
      "           Using gblinear booster with shotgun updater is nondeterministic as\n",
      "           it uses Hogwild algorithm.\n",
      "\n",
      "    missing : float, default np.nan\n",
      "        Value in the data which needs to be present as a missing value.\n",
      "    num_parallel_tree: int\n",
      "        Used for boosting random forest.\n",
      "    monotone_constraints : str\n",
      "        Constraint of variable monotonicity.  See tutorial for more\n",
      "        information.\n",
      "    interaction_constraints : str\n",
      "        Constraints for interaction representing permitted interactions.  The\n",
      "        constraints must be specified in the form of a nest list, e.g. [[0, 1],\n",
      "        [2, 3, 4]], where each inner list is a group of indices of features\n",
      "        that are allowed to interact with each other.  See tutorial for more\n",
      "        information\n",
      "    importance_type: string, default \"gain\"\n",
      "        The feature importance type for the feature_importances\\_ property:\n",
      "        either \"gain\", \"weight\", \"cover\", \"total_gain\" or \"total_cover\".\n",
      "    gpu_id :\n",
      "        Device ordinal.\n",
      "    validate_parameters :\n",
      "        Give warnings for unknown parameter.\n",
      "\n",
      "    \\*\\*kwargs : dict, optional\n",
      "        Keyword arguments for XGBoost Booster object.  Full documentation of\n",
      "        parameters can be found here:\n",
      "        https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst.\n",
      "        Attempting to set a parameter via the constructor args and \\*\\*kwargs\n",
      "        dict simultaneously will result in a TypeError.\n",
      "\n",
      "        .. note:: \\*\\*kwargs unsupported by scikit-learn\n",
      "\n",
      "            \\*\\*kwargs is unsupported by scikit-learn.  We do not guarantee\n",
      "            that parameters passed via this argument will interact properly\n",
      "            with scikit-learn.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(class_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa850b7b-c319-41a2-bb62-7174f3a772a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
